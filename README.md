# OpenStack on ThinkSystem Minis (Kolla‑Ansible, 2024.1 Caracal)

Minimal, reproducible repo to deploy a **multi‑node OpenStack** on small Lenovo ThinkSystem Minis (or similar) using **Kolla‑Ansible** (containerised services).

> Tested layout: 1× control + 1–2× compute, Ubuntu 22.04 LTS, flat provider network on a local bridge `prov0`, Cinder LVM backed by a loop file (lab‑only).

---

## Topology (example)

- **mini-ctrl** (controller): API services, DB, MQ, Neutron, Cinder‑LVM, Horizon
- **mini-cmp1..N** (compute): Nova compute + Neutron agent
- Networks:
  - **Management**: e.g., `192.168.50.0/24` on NIC `enp1s0`
  - **Provider** (flat): bridge `prov0` attached to a trunk/untagged port, e.g., `192.168.60.0/24`

Update interface names, subnets, and IPs to match your environment.

---

## Quick start

```bash
# 0) Prepare your minis: Ubuntu 22.04, passwordless sudo, and SSH keys from your laptop.
#    Update inventory/multinode.ini and kolla/globals.yml (NIC names, VIP, etc.)

# 1) On your laptop / admin host
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2) Check connectivity
ansible -i inventory/multinode.ini all -m ping

# 3) Bootstrap hosts (packages, provider bridge, Cinder VG file on controller)
ansible-playbook playbooks/bootstrap.yml

# 4) Deploy OpenStack with Kolla‑Ansible
ansible-playbook playbooks/deploy.yml

# 5) Source admin credentials on the controller (or copy them locally)
#    Typically created at /etc/kolla/admin-openrc.sh on the localhost where you ran kolla-ansible post-deploy.
#    If you ran from your laptop, the file is in ./etc/kolla/admin-openrc.sh (relative to this repo).
source etc/kolla/admin-openrc.sh

# 6) Sanity check & demo
openstack service list
./scripts/demo.sh
```

Horizon is exposed on the VIP you set in `kolla/globals.yml` (default `http://192.168.50.20/`). Credentials are in `etc/kolla/admin-openrc.sh` (generated by post‑deploy).

---

## Repo layout

```
openstack-on-minis/
├─ ansible.cfg
├─ requirements.txt
├─ .gitignore
├─ README.md
├─ LICENSE
├─ inventory/
│  ├─ multinode.ini
│  └─ host_vars/
│     ├─ mini-ctrl.yml
│     └─ mini-cmp1.yml
├─ kolla/
│  ├─ globals.yml
│  └─ passwords.yml.example
├─ playbooks/
│  ├─ bootstrap.yml
│  ├─ deploy.yml
│  └─ postdeploy.yml
└─ scripts/
   ├─ gen-passwords.sh
   └─ demo.sh
```

---

## Notes & tips

- **Passwords**: generate `kolla/passwords.yml` with `./scripts/gen-passwords.sh` (don’t commit it).
- **Cinder storage**: this lab uses a **loop‑backed LVM VG** for simplicity. Replace with a real disk/VG for realistic testing.
- **Provider network**: we create a Linux bridge `prov0`. If you want VLAN provider nets, adjust Neutron ML2 config and physical mapping accordingly.
- **Reset**: re‑run `kolla-ansible destroy --yes-i-really-really-mean-it` then `deploy` (be careful – this wipes data).

---

## Common issues

- **Wrong NIC names**: update `network_interface` and `neutron_external_interface` in `kolla/globals.yml`.
- **VIP not reachable**: ensure the VIP (`kolla_internal_vip_address`) is within your mgmt subnet and unused; Keepalived will float it on the control host(s).
- **Cinder volume create fails**: confirm the VG `cinder-volumes` exists on the controller (`vgdisplay`).

---

## Next steps

- Add extra computes to `[compute]` and re‑deploy.
- Enable more services in `kolla/globals.yml` (Octavia, Barbican, etc.).
- Configure VLAN provider nets or VXLAN overlays.
---

## Rocky Linux notes

- This variant sets `kolla_base_distro: "rocky"` and `kolla_container_engine: "docker"` in `kolla/globals.yml`.
- `playbooks/bootstrap.yml` is now OS-agnostic and configures SELinux **permissive**, disables `firewalld` (lab only), loads bridge/overlay modules, and sets IP forwarding.
- The Neutron agent is set to **linuxbridge** for simplicity. The provider interface is `prov0` (Linux bridge). If you prefer Open vSwitch, set `neutron_plugin_agent: "openvswitch"` and use `neutron_bridge_name: "br-ex"` instead of `neutron_external_interface`.
- Ensure your Rocky hosts have passwordless sudo and match the NIC names in `kolla/globals.yml` (`network_interface`) and your inventory SSH user (often `rocky` or `root`).
